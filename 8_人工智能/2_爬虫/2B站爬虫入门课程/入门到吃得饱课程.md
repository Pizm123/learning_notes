[TOC]

## 1、爬虫合法性探究

### 1.1、爬虫合法性

* 在法律中不被禁止
  * 不能干扰被爬取网站的正常运营
  * 不能爬取受法律保护的特定类型的数据或信息

## 2、爬虫初始探入

### 2.1、爬虫在使用场景中的分类

* 通用爬虫
  * 抓取系统重要组成部分，抓取的是一整张页面数据
* 聚焦爬虫
  * 是建立在通用爬虫基础之上。抓取的是页面中特定的局部内容。
* 增量式爬虫
  * 检测网站中数据更新的情况，只会抓取网站中最新更新出来的数据 

* 爬虫的矛与盾
  * 反爬机制
    * 门户网站，可以通过指定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取
  * 反反爬策略
    * 爬虫程序可以通过指定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站数据

### 2.2、robots.txt协议

* 君子协议：规定了网站中哪些数据可以被爬虫爬取，哪些数据不可以被爬取。

### 2.3、http与https协议

* http协议

  * 概念：就是服务器和客户端进行数据交互的一种形式。
  * 常用请求头信息
    * User-Agent：请求载体的身份标识
    * Connection：请求完毕以后，是断开连接还是保持连接
  * 常用响应头信息
    * Content-Type：服务器响应回客户端的数据类型

* https协议

  * 安全的超文本传输协议

  * 通过证书密钥加密

* 加密方式

  * 对称加密
  * 非对称加密
  * 证书密钥加密

## 3、requests模块

### 3.1、[resuests简介](../1黑马爬虫入门/python爬虫入门.md)

### 3.2、实战巩固

#### 3.2.1、简易网页采集器

```python
# UA：User-Agent（请求载体的身份标识）
# UA检测：门户网站的服务器会检测对应请求的载体标识，如果检测到请求暂替身份标识为某一款浏览器，说明该请求是一个正常的请求。
# UA伪装：让爬虫对应的请求载体身份标识伪装成某一浏览器
```

```python
import requests

if __name__ == '__main__':
    # url
    url = "https://www.sogou.com/web"
    kw = input('enter a word:')
    # 参数
    param = {
        'query': kw
    }
    # 请求头
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/103.0.0.0 Safari/537.36 '
    }
    # 发起请求
    response = requests.get(url=url, params=param, headers=headers)

    page_text = response.text

    with open(kw + ".html", 'w', encoding='utf8') as fp:
        fp.write(page_text)
    print(f"{kw}保存成功")

```

#### 3.2.2、破解百度翻译

* post请求

* 响应数据是一个json

* ```python
  import requests
  import json
  
  if __name__ == '__main__':
      # url
      post_url = "https://fanyi.baidu.com/sug"
      kw = input("请输入要翻译的单词：")
      # 参数
      param = {
          'kw': kw
      }
      # 请求头
      headers = {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                        'Chrome/103.0.0.0 Safari/537.36 '
      }
      # 发起请求
      response = requests.post(post_url, data=param, headers=headers)
  
      # 如果确认响应数据是json格式的，可以使用.json()方法获取对象
      page_text = response.json()['data']
  
      print("翻译完成")
  
      for i in page_text:
          print(f'{i["k"]}:{i["v"]}')
  
  ```

#### 3.2.3、豆瓣电影

```python
import requests
from tqdm import tqdm
import time

page_text = None


def load():
    # url
    post_url = "https://movie.douban.com/j/chart/top_list"
    # 参数
    param = {
        'type': '24',
        'interval_id': '100:90',
        'action': '',
        'start': '1',  # 开始个数
        'limit': '20'  # 取出个数
    }
    # 请求头
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/103.0.0.0 Safari/537.36 '
    }
    # 发起请求
    response = requests.get(url=post_url, params=param, headers=headers)

    # 如果确认响应数据是json格式的，可以使用.json()方法获取对象
    global page_text
    page_text = response.json()

    for i in tqdm(range(11), "正在获取电影数据。。。"):
        time.sleep(0.2)


def show_res():
    print("电影名称\t电影类型\t地区")
    for i in page_text:
        types = ''
        for a in i['types']:
            types += a + '、'
        types = types.strip('、')
        regions = ''
        for a in i['regions']:
            regions += a + '、'
        regions = regions.strip('、')
        print('{str:一<9}'.format(str=str(i["title"])), end='\t')
        print('{str:一<16}'.format(str=types), end='\t')
        print('{str:一<10}'.format(str=regions), end='\t')
        print('')


if __name__ == '__main__':
    while True:
        print("----某瓣电影排行榜信息爬虫----")
        print("1、开始获取")
        print("2、显示结果")
        print("3、退出系统")
        menu_num = int(input("请输入功能序号："))
        if menu_num == 1:
            load()
        elif menu_num == 2:
            show_res()
        elif menu_num == 3:
            print("系统退出成功")
            break
```

#### 3.2.4、肯德基餐厅地址

```python
import requests
import json

if __name__ == '__main__':
    # url
    post_url = "http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx"
    # 参数
    param = {
        'cname': '',
        'pid': '',
        'keyword': '东莞',
        'pageIndex': '1',
        'pageSize': '10'
    }
    # 请求头
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/103.0.0.0 Safari/537.36 '
    }
    # 发起请求
    response = requests.post(url=post_url, params={'op': 'keyword'}, data=param, headers=headers)

    # 如果确认响应数据是json格式的，可以使用.json()方法获取对象
    page_text = response.text

    res = json.loads(page_text)['Table1']

    # print(res)

    for i in res:
        print(f'名称：{str(i["storeName"]).ljust(14, " ")}\t地址：{str(i["addressDetail"]).ljust(40, " ")}')

```

## 4、数据解析

### 4.1、聚焦爬虫

* 编码流程
  * 指定url
  * 发起请求
  * 获取响应数据
  * 数据解析
  * 持久化存储

### 4.2、数据解析分类

* 正则
* bs4
* xpath

### 4.3、数据解析原理概述

* 解析的局部的文本内容都在标签的之间或者标签对应的属性中进行存储
* 进行指定标签的定位
* 标签或者标签对应的属性中存储的数据值进行提取（提取）

### 4.4、bs4实战演练

#### 4.4.1、小说爬取

```python
import requests
import json
from bs4 import BeautifulSoup

if __name__ == '__main__':
    # url
    url = "https://www.shicimingju.com/book/sanguoyanyi.html"
    # 请求头
    headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/103.0.0.0 Safari/537.36 '
    }
    # 发起请求
    response = requests.get(url=url, headers=headers)
    page_text = response.content.decode()

    # 数据解析
    soup = BeautifulSoup(page_text, 'lxml')

    li_list = soup.select('.book-mulu > ul > li')

    fp = open('.sanguo.txt', 'w', encoding='utf8')

    for li in li_list:
        title = li.a.string
        detail_url = 'https://www.shicimingju.com' + li.a['href']
        # 对详情页发起请求
        detail_text = requests.get(detail_url, headers=headers).content.decode()
        detail_soup = BeautifulSoup(detail_text, 'lxml')
        div_tag = detail_soup.find('div', class_='chapter_content')
        content = div_tag.text
        fp.write(title + ':' + content + '\n')
        print(title, '爬取成功')

    fp.close()
```

### 4.5、xpath解析

* 实例化一个xtree对象，且需要将被解析的页面源码数据加载到该对象中

* 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获

* 环境安装 pip install lxml

* 如果实例化一个etree对象

  * 将本地html文档中的源码数据加载到etree对象中

    * ``` etree.parse(filePath)```

  * 可以将从互联网中获取的源码数据加载到该对象中

    * ```etree.HTML('page_text')```

  * xpath('xpath表达式')

    * /表示的是从根节点开始定位。表示的是一个层级

      ```python
      # r = tree.xpath('/html/head/title')
      ```

    * //表示多个层级。可以表示从任意位置开始定位

      ```python
      r = tree.xpath('/html//div')
      r = tree.xpath('//div')
      ```

    * 属性定位

      ```python
      # tag[@attrName="attrValue"]
      r = tree.xpath('//div[@class=""]')
      ```

    * 索引定位

      ```python
      r = tree.xpath('//div[@class=""]/p[3]') # 索引从1开始
      ```

    * 取文本

      ```python
      # /test() 获取直系文本
      # //test() 获取标签内所有文本
      r = tree.xpath('//div[@class=""]/p[3]/a/test()')
      ```

    * 取属性

      ```python
      # /attrName 获取属性值
      r = tree.xpath('//div[@class=""]/img/@src')
      ```

### 4.6、xpath实战演练

#### 4.6.1、58二手房信息

```python
from lxml import etree
import requests

if __name__ == '__main__':
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 '
    }
    page_text = requests.get('https://dg.58.com/ershoufang/', headers=headers).text

    # print(page_text)
    # # 数据解析
    tree = etree.HTML(page_text)
    div_list = tree.xpath('//*[@id="esfMain"]/section/section[3]/section[1]/section[2]/div')
    for i in div_list:
        title = i.xpath('./a/div[2]/div[1]/div[1]/h3/text()')[0]
        print(title)
```

#### 4.6.2、彼岸图网图片数据解析

```python
from lxml import etree
import requests
import os
from tqdm import tqdm
import time

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 ',
    'referer': 'https://pic.netbian.com/tupian/24232.html'
}
home_url = 'https://pic.netbian.com'


def down_image(img_src, img_path):
    """
    根据url下载图片
    :param img_src: 图片url地址
    :param img_path: 图片保存路径
    :return:
    """
    resp = requests.get(url=img_src, headers=headers)
    img_data = resp.content
    with open(img_path, 'wb') as fp:
        fp.write(img_data)
        # print(img_path, '下载成功')


def load_pic(page_url, i):
    """
    解析每张页面内容，获取图片url
    :param page_url:
    :param i:
    :return:
    """
    response = requests.get(page_url, headers=headers)
    response.encoding = 'gbk'
    page_text = response.text

    # print(page_text)
    # # 数据解析
    tree = etree.HTML(page_text)
    li_list = tree.xpath('//*[@id="main"]/div[3]/ul/li')

    if not os.path.exists('./meinv'):
        os.mkdir('./meinv/')

    for li in tqdm(li_list, '第%s页小姐姐图片下载进度' % i):
        time.sleep(0.1)
        # 详情页url
        detail_url = home_url + li.xpath('./a/@href')[0]
        # 图片名称
        img_name = li.xpath('./a/img/@alt')[0] + '.jpg'
        # 请求详情页数据
        detail_page = requests.get(url=detail_url, headers=headers).text
        # 解析数据
        detail_tree = etree.HTML(detail_page)
        img_src = home_url + detail_tree.xpath('//*[@id="img"]/img/@src')[0]
        img_path = './meinv/' + img_name
        # 下载图片
        down_image(img_src, img_path)


def batch_down():
    """
    遍历页面
    :return:
    """
    # 第一页下载
    load_pic(home_url + '/4kmeinv/', 1)
    # 第二页之后下载
    for i in range(2, 6):
        page_url = home_url + '/4kmeinv/index_%s.html' % i
        load_pic(page_url, i)


def run():
    while True:
        print("--小姐姐图片下载爬虫--")
        print("1、开始下载")
        print("2、退出系统")
        menu_id = int(input("请输入功能编号："))
        if menu_id == 1:
            batch_down()
        elif menu_id == 2:
            print("系统退出成功")
            break
        else:
            print("功能编号不正确，请重新输入")


if __name__ == '__main__':
    run()
```

#### 4.6.3、全国城市名称

```python
from lxml import etree
import requests
import os

if __name__ == '__main__':
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 '
    }
    # 请求输入
    response = requests.get('https://www.aqistudy.cn/historydata/', headers=headers)
    response.encoding = 'utf8'
    page_text = response.text

    # print(page_text)
    # # 数据解析
    tree = etree.HTML(page_text)
    
    # 全部城市
    all_li_list = tree.xpath('//div[@class="bottom"]/ul/li/a | //div[@class="bottom"]/ul/div[2]/li/a')
    all_city_names = []

    for a in all_li_list:
        city_name = a.xpath('./text()')[0]
        all_city_names.append(city_name)

    print(all_city_names)
```

#### 4.6.4、简历模板下载

```python
from lxml import etree
import requests
import os
import time

if __name__ == '__main__':
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36 '
    }
    # 请求数据
    response = requests.get('https://sc.chinaz.com/jianli/free.html', headers=headers)
    response.encoding = 'utf8'
    page_text = response.text

    # # 数据解析
    tree = etree.HTML(page_text)

    # 免费简历模板列表
    jl_list = tree.xpath('//div[@id="main"]/div/div')

    if not os.path.exists('./jl'):
        os.mkdir('./jl')

    for div in jl_list:
        time.sleep(0.3)
        # 简历模板详情页url地址
        jl_detail_url = 'https:' + div.xpath('./a/@href')[0]
        # 简历名称
        jl_name = div.xpath('./a/img/@alt')[0]
        # 简历路径
        jl_path = 'jl/' + jl_name + '.rar'
        # 请求简历详情页
        jl_detail_page = requests.get(jl_detail_url, headers=headers).text
        detail_tree = etree.HTML(jl_detail_page)

        # # 简历url
        jl_url = detail_tree.xpath('//ul[@class="clearfix"]/li[1]/a/@href')[0]
        # 简历文件数据
        jl_data = requests.get(jl_url, headers=headers).content
        # 保存简历数据
        with open(jl_path, 'wb') as fp:
            fp.write(jl_data)
        print(jl_name, "下载完成")
```

## 5、反反爬机制

### 5.1、验证码识别简介

* 反爬机制：验证码
* 验证码识别：识别验证码图片中的数据，用于模拟登陆操作
* 识别验证码的方法
  * 人工肉眼识别
  * 第三方自动识别

### 5.2、cookie：

* 手动cookie处理

* 自动处理

  * cookie的来源：模拟登录post请求后，由服务端创建

  * session会话对象

    * 可以进行请求发送
    * 如果请求过程中产生了cookie，则cookie会被自动存储/携带到session对象中

  * 过程

    * 创建session对象

      ```python
      session = requests.Session()
      ```

    * 使用session对象进行模拟登录post请求发送（cookie会被存储到session中）

      ```python
      session.post()
      ```

    * session对象对主页发送请求

      ```python
      session.get()
      ```

### 5.3、代理

* 破解封IP这种反爬机制
* 什么是代理：
  * 代理服务器
* 代理的作用
  * 突破自身IP访问的限制
  * 隐藏自身真实的IP
* 代理相关网站
  * 快代理
  * 西祠代理

## 6、高性能异步爬虫

* 目的：在爬虫中使用异步实现高性能的数据爬取操作

### 6.1、多线程

* 好处：可以为相关阻塞操作单独开启线程或者进程，阻塞操作就可以异步执行
* 弊端：无法无限制的开启多线程或者多进程

### 6.2、线程池

* 好处：可以降低系统对进程或者线程创建和销毁的频率，从而很好的降低系统的开销

* 弊端：池中线程或者进程的数量是有上限的

* ```python
  from multiprocessing.dummy import Pool
  pool = Pool(4)
  pool.map(get_page, name_list)  # 将列表中每一个元素交给map进行处理
  ```

* 示例

  ```python
  import time
  from multiprocessing.dummy import Pool
  
  start_time = time.time()
  
  
  def get_page(str1):
      print("正在下载：", str1)
      time.sleep(2)
      print('下载成功', str1)
  
  
  name_list = ['1', '2', '3', '4']
  
  pool = Pool(4)
  pool.map(get_page, name_list)  # 将列表中每一个元素交给map进行处理
  
  end_time = time.time()
  print('%d second' % (end_time - start_time))
  ```

### 6.3、单线程+异步协程（推荐）

* event_loop：事件循环，相当于一个无限循环，可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行

* coroutine：协程对象，可以将协程对象注册到事件循环中，它会被事件循环调用。可以使用asyns关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象

* task：任务，它是对协程对象的进一步封装，包含了任务的各个状态

* future：代表将来执行或还没有执行的任务，实际上和task没有本质区别

* async 定义一个协程

* await 用来挂起阻塞方法的执行

#### 6.3.1、示例代码

* 单任务协程

  ```python
  import asyncio
  
  
  async def request(url):
      print("正在请求的url是", url)
      print("请求成功")
      return url
  
  
  # async修饰的函数，调用之后返回一个协程对象
  c = request("www.baidu.com")
  
  
  # # 创建一个事件循环对象
  # loop = asyncio.get_event_loop()
  #
  # # 将协程对象注册到loop中，然后启动loop
  # loop.run_until_complete(c)
  
  # task的使用
  # loop = asyncio.get_event_loop()
  # # 基于loop创建一个task对象
  # task = loop.create_task(c)
  # print(task)
  #
  # loop.run_until_complete(task)
  # print(task)
  
  # # future的使用
  # loop = asyncio.get_event_loop()
  # task = asyncio.ensure_future(c)
  # print(task)
  # loop.run_until_complete(task)
  # print(task)
  
  def callback_func(task):
      # result返回的就是任务对象中封装的协程对象的对应函数的返回值
      print(task.result())
  
  
  # 绑定回调
  loop = asyncio.get_event_loop()
  task = asyncio.ensure_future(c)
  # 将回调函数绑定到任务对象中
  task.add_done_callback(callback_func)
  loop.run_until_complete(task)
  ```

* 多任务协程

  ```python
  import asyncio
  import time
  
  
  async def request(url):
      print("正在下载", url)
      # 在异步协程中如果出现了同步模块相关的代码，那么就无法实现异步
      # time.sleep(2)
      # 当在asyncio中遇到阻塞操作必须进行手动挂起：await
      await asyncio.sleep(2)
      print("下载完成", url)
  
  
  starttime = time.time()
  
  urls = [
      'www.baidu.com',
      'www.sougou.com',
      'www.goubanjia.com'
  ]
  # 任务列表，存放多个任务对象
  tasks = []
  for url in urls:
      c = request(url)
      task = asyncio.ensure_future(c)
      tasks.append(task)
  
  loop = asyncio.get_event_loop()
  # 任务列表放在asyncio.wait中
  loop.run_until_complete(asyncio.wait(tasks))
  
  print(time.time() - starttime)
  ```

* 协程请求例子

  ```python
  from flask import Flask
  import time
  # web服务
  app = Flask(__name__)
  
  
  @app.route('/bobo')
  def index_bobo():
      time.sleep(2)
      return 'Hello bobo'
  
  
  @app.route('/jay')
  def index_jay():
      time.sleep(2)
      return 'Hello jay'
  
  
  @app.route('/tom')
  def index_tom():
      time.sleep(2)
      return 'Hello tom'
  
  
  if __name__ == '__main__':
      app.run(threaded=True)
  ```

  客户端

  ```python
  import asyncio
  import time
  import requests
  import aiohttp
  
  
  async def get_page(url):
      async with aiohttp.ClientSession() as session:
          # get()、post()
          # headers,params,data,proxy='http://ip:port'
          async with await session.get(url) as response:
              # text()返回字符串形式的响应数据
              # read()返回二进制形式的响应数据
              # json() 返回json对象
              # 获取响应数据操作之前一定要使用await进行手动挂起
              page_text = await response.text()
              print(page_text)
  
  
  start = time.time()
  urls = [
      'http://127.0.0.1:5000/bobo',
      'http://127.0.0.1:5000/jay',
      'http://127.0.0.1:5000/tom'
  ]
  # 任务列表，存放多个任务对象
  tasks = []
  for url in urls:
      c = get_page(url)
      task = asyncio.ensure_future(c)
      tasks.append(task)
  
  loop = asyncio.get_event_loop()
  # 任务列表放在asyncio.wait中
  loop.run_until_complete(asyncio.wait(tasks))
  print(time.time() - start)
  ```

## 7、selenium模块的基本使用

### 7.1、selenium简介

* 便捷的获取网站中动态加载的数据

* 便捷的实现模拟登录

* 什么是selenium

  * 基于浏览器自动化的一个模块

### 7.2、selenium使用流程

1. 环境安装 pip install selenium

2. 下载一个浏览器的驱动程序

3. 实例化一个浏览器对象

* ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  
  s = Service('D:\worktools\chromedriver_win32\chromedriver.exe')
  # 实例化一个浏览器对象，传入浏览器驱动位置
  bro = webdriver.Chrome(service=s)
  bro.get('https://www.baidu.com')
  
  # 获取浏览器当前页面的源码数据
  page_text = bro.page_source
  
  print(page_text)
  ```

### 7.3、基于浏览器的自动化操作代码

* 发起请求：get(url)

* 标签定位：find系列的方法

* 标签交互：send_keys('xxx')

* 执行js程序：excute_script('jsCode')

* 前进,后退：back()，forward()

* 关闭浏览器：quit()

* 截图方法：save_screenshot('aa.png')

* 代码示例

  ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  from selenium.webdriver.common.by import By
  from time import sleep
  
  s = Service('./chromedriver.exe')
  # 打开浏览器
  bro = webdriver.Chrome(service=s)
  # 打开淘宝页面
  bro.get("https://www.taobao.com/")
  # 标签定位
  search_input = bro.find_element(by=By.ID, value='q')
  # 标签交互
  search_input.send_keys("IPhone")
  # 定位按钮
  btn = bro.find_element(by=By.CLASS_NAME, value='btn-search')
  
  # 执行一组js程序
  bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
  sleep(2)
  # btn.click()
  
  bro.get('https://www.baidu.com')
  sleep(2)
  # 返回方法
  bro.back()
  sleep(2)
  # 前进方法
  bro.forward()
  sleep(5)
  # 关闭浏览器
  bro.quit()
  ```

### 7.4、selemium处理iframe

* 如果定位的标签存在于iframe标签中，直接定位无法定位

* 通过switch_to.frame('iframe')，切换浏览器标签定位的作用域

#### 7.4.1、动作链

* 动作链(拖动)

  * 实例化一个动作链对象 action = ActionChains()
  * click_and_hold(div)：长按且点击操作
  * move_by_offset(x,y)
  * perform()让动作链立即执行
  * action.release()释放动作链对象

* ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  from selenium.webdriver.common.by import By
  from time import sleep
  from selenium.webdriver import ActionChains
  
  s = Service('./chromedriver.exe')
  # 打开浏览器
  bro = webdriver.Chrome(service=s)
  
  bro.get("https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable")
  sleep(2)
  bro.switch_to.frame('iframeResult')
  div = bro.find_element(by=By.ID, value='draggable')
  # 动作链
  action = ActionChains(bro)
  # 点击长按指定的标签
  action.click_and_hold(div)
  # 偏移
  for i in range(5):
      # perform()立即执行动作链操作
      action.move_by_offset(17, 0).perform()
      sleep(0.3)
  # 释放动作链
  action.release()
  # sleep(5)
  # 关闭浏览器
  bro.quit()
  ```

#### 7.4.2、模拟登录案例

```python
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from time import sleep
from selenium.webdriver import ActionChains

s = Service('./chromedriver.exe')
# 打开浏览器
bro = webdriver.Chrome(service=s)
# 打开qq空间
bro.get("https://qzone.qq.com/")
# 切换作用域
bro.switch_to.frame('login_frame')
# 定位到 密码登录的标签
a = bro.find_element(By.ID, 'switcher_plogin')
# 点击标签
a.click()
user = bro.find_element(By.ID, 'u')
passwd = bro.find_element(By.ID, 'p')
sleep(2)
user.send_keys('908230613')
passwd.send_keys('******')
sleep(2)
btn = bro.find_element(By.ID, 'login_button')
btn.click()

sleep(5)
# 关闭浏览器
bro.quit()
```



### 7.5、无头浏览器

* 无可视化界面

  ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  from selenium.webdriver.common.by import By
  from time import sleep
  from selenium.webdriver import ActionChains
  from selenium.webdriver.chrome.options import Options
  
  chrome_options = Options()
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--disable-gpu')
  
  s = Service('./chromedriver.exe')
  # 打开浏览器
  bro = webdriver.Chrome(service=s, chrome_options=chrome_options)
  # 打开qq空间
  bro.get("https://www.baidu.com/")
  print(bro.page_source)
  
  sleep(5)
  # 关闭浏览器
  bro.quit()
  ```

### 7.5、规避selenium被检测到的风险

* ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  from time import sleep
  # 实现无可视化页面
  from selenium.webdriver.chrome.options import Options
  # 实现规避检测
  from selenium.webdriver import ChromeOptions
  
  # 实现无可视化页面
  chrome_options = Options()
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--disable-gpu')
  
  # 实现规避检测
  options = ChromeOptions()
  options.add_experimental_option('excludeSwitches', ['enable-automation'])
  
  s = Service('./chromedriver.exe')
  # 打开浏览器
  bro = webdriver.Chrome(service=s, chrome_options=chrome_options, options=options)
  # 打开qq空间
  bro.get("https://www.baidu.com/")
  print(bro.page_source)
  
  sleep(5)
  # 关闭浏览器
  bro.quit()
  ```

### 7.6、12306模拟登录

* 超级鹰：验证码识别平台

  * https://www.chaojiying.com/user/

* ```python
  from selenium import webdriver
  from selenium.webdriver.chrome.service import Service
  from time import sleep
  # 实现规避检测
  from selenium.webdriver import ChromeOptions
  from selenium.webdriver.common.by import By
  from selenium.webdriver import ActionChains
  
  # 实现规避检测
  options = ChromeOptions()
  options.add_experimental_option('excludeSwitches', ['enable-automation'])
  
  s = Service('./chromedriver.exe')
  # 打开浏览器
  bro = webdriver.Chrome(service=s, options=options)
  # 打开12306
  bro.get("https://kyfw.12306.cn/otn/resources/login.html")
  tel = bro.find_element(By.ID, 'J-userName')
  tel.send_keys('17854172703')
  sleep(1)
  passwd = bro.find_element(By.ID, 'J-password')
  passwd.send_keys('*******')
  sleep(1)
  btn = bro.find_element(By.ID, 'J-login')
  btn.click()
  script = 'Object.defineProperty(navigator, "webdriver", {get: () => false,});'
  bro.execute_script(script)
  sleep(1)
  # 滑块标签
  hk = bro.find_element(By.XPATH, '//*[@id="nc_1_n1z"]')
  print(hk)
  # 动作链
  action = ActionChains(bro)
  # 点击并按住滑块
  action.click_and_hold(hk)
  for i in range(5):
      action.move_by_offset(72, 0).perform()
      sleep(0.1)
  action.release()
  
  sleep(5)
  # 关闭浏览器
  bro.quit()
  
  ```

## 8、scrapy框架

### 8.1、框架简介

* 什么是框架
  * 就是一个集成了很多功能，并且具有很强的通用性的项目模板

* 如何学习框架
  * 学习框架封装的各种功能的详细用法

* 什么是scrapy
  * 爬虫中封装好的一个明星框架
  * 功能：
    * 高性能的持久化存储
    * 异步的数据下载
    * 高性能的数据解析
    * 分布式

### 8.2、scrapy框架的基本使用

#### 8.2.1、环境的安装

* mac or linux pip install scrapy
* windows
  * pip install wheel
  * 下载twisted 下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/
  * 安装twisted ：pip install Twisted-20.3.0-cp37-cp37m-win_amd64.whl
  * pip install pywin32
  * pip install scrapy

#### 8.2.2、创建一个工程

* 命令：scrapy startproject xxxPro

#### 8.2.3、工程目录结构

* 项目同名目录
  * spiders：爬虫文件夹
    * ```__init__.py```
  * ```__init__.py```
  * items.py
  * middlewares.py
  * pipelines.py
  * settings.py：配置文件
  
    ```python
    # 遵从robots协议
    ROBOTSTXT_OBEY = True
    # 日志级别
    LOG_LEVEL = 'ERROR'
    ```
* scrapy.cfg

#### 8.2.4、项目命令

* 在spiders子目录创建爬虫文件
  * 命令：```scrapy genspider spiderName www.xxx.com```

* 执行工程
  * 命令行：

    ```cmd
    # 执行工程
    scrapy crawl spiderName
    # 不打印日志
    scrapy crawl spiderName --nolog
    ```

### 8.3、scrapy的数据解析

* 爬虫脚本文件类中的def parse(self, response):方法用于数据解析
* response.xpath()返回一个Selector对象列表，extract()方法将Selector类型的对象中的data属性取出来
* 代码示例

```python
import scrapy

class FirstSpider(scrapy.Spider):
    # 爬虫文件名称，爬虫源文件的一个唯一标识
    name = 'first'
    # 允许的域名：用来限定start_urls列表中哪些url可以进行请求发送,通常不适用
    # allowed_domains = ['www.baidu.com']
    # 起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送
    start_urls = ['https://www.baidu.com/']

    def parse(self, response):
        """
        用于数据解析
        :param response: 请求成功后对应的响应对象
        :return:
        """
        print(response)
        # xpath返回的是列表，列表元素是Selector类型的对象
        # extract()方法将Selector类型的对象中的data属性取出来
        title = response.xpath('//*[@id="s-top-left"]/a[1]/text()')[0].extract()
        print(title)
        # 列表可以使用extract()方法 or extract_first()方法
        title = response.xpath('//*[@id="s-top-left"]/a[1]/text()').extract()[0]
        print(title)
```

### 8.4、scrapy持久化存储 

#### 8.4.1、基于终端指令

* 要求：只可以将parse方法的返回值存储到本地的文本文件中

* 注意：

  * 持久化存储对应的文本文件类型只可以为：'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'类型
  * 返回值类型只能是item对象 scrapy.Item的子类

* 指令：``` scrapy crawl first -o ./baidu.csv```

* 优缺点

  * 优点：简洁高效便捷
  * 缺点：局限性比较强

#### 8.4.2、终端指令代码示例

  ```python
  import scrapy
  
  
  class Product(scrapy.Item):
      name = scrapy.Field()
  
  
  class FirstSpider(scrapy.Spider):
      # 爬虫文件名称，爬虫源文件的一个唯一标识
      name = 'first'
      # 允许的域名：用来限定start_urls列表中哪些url可以进行请求发送,通常不适用
      # allowed_domains = ['www.baidu.com']
      # 起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送
      start_urls = ['https://www.baidu.com/']
  
      def parse(self, response):
          """
          用于数据解析
          :param response: 请求成功后对应的响应对象
          :return:
          """
          print(response)
          # xpath返回的是列表，列表元素是Selector类型的对象
          # extract()方法将Selector类型的对象中的data属性取出来
          title = response.xpath('//*[@id="s-top-left"]/a/text()').extract()
          print(title)
  
          product = Product()
          product['name'] = title
          return product
  ```

#### 8.4.3、基于管道

* 管道优点：通用性强

* 编码流程

  * 数据解析

  * 将解析的数据封装存储到item类型的对象

    ```python
    # 爬虫文件
    	def parse(self, response):
            """
            用于数据解析
            :param response: 请求成功后对应的响应对象
            :return:
            """
    
            item = ScrapyDemoItem()
            item['author'] = 'pizm'
            item['content'] = '这是一个item测试demo'
            yield item
    ```

  * 将item类型的对象提交给管道进行持久化存储的操作

    ```yield item```

  * 在管道类的process_item中要将其接受到的item对象存储的数据进行持久化存储操作

    ```python
    # Define your item pipelines here
    #
    # Don't forget to add your pipeline to the ITEM_PIPELINES setting
    # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
    
    
    # useful for handling different item types with a single interface
    from itemadapter import ItemAdapter
    
    
    class ScrapyDemoPipeline:
        fp = None
    
        # 重写父类方法：只在开始爬虫的时候被调用一次
        def open_spider(self, spider):
            print("开始爬虫")
            self.fp = open('./baidu.txt', 'w', encoding='utf8')
    
        def process_item(self, item, spider):
            """
            用来处理item类型对象
            该方法的可以接收爬虫文件提交过来的item对象
            每接收到一个item对象就会被调用一次
            :param item:
            :param spider:
            :return:
            """
            author = item['author']
            content = item['content']
            self.fp.write(author + ':' + content)
            return item
    
        # 重写父类方法：只在开始爬虫的时候被调用一次
        def close_spider(self, spider):
            print("结束爬虫")
            self.fp.close()
    ```
  
  * 在配置文件中开启管道

    ```python
    # settings.py
    ITEM_PIPELINES = {
       'scrapy_demo.pipelines.ScrapyDemoPipeline': 300,
        # 300表示优先级，数值越小优先级越高
    }
    ```
  
* 如何同时将爬取到的数据保存到文件和mysql中

  * 管道文件中定义两个管道类，每个管道类对应存储数据到一种平台
    * yield item会将item提交给优先级最高的管道类
    * 第一个管道类的process_item()方法最后 return item 表示将item 传递到下一个即将执行的管道类
    
  * ```python
    # Define your item pipelines here
    #
    # Don't forget to add your pipeline to the ITEM_PIPELINES setting
    # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
    
    
    # useful for handling different item types with a single interface
    from itemadapter import ItemAdapter
    import pymysql
    
    
    class ScrapyDemoPipeline:
        fp = None
    
        # 重写父类方法：只在开始爬虫的时候被调用一次
        def open_spider(self, spider):
            print("开始爬虫")
            self.fp = open('./baidu.txt', 'w', encoding='utf8')
    
        def process_item(self, item, spider):
            """
            用来处理item类型对象
            该方法的可以接收爬虫文件提交过来的item对象
            每接收到一个item对象就会被调用一次
            :param item:
            :param spider:
            :return:
            """
            author = item['author']
            content = item['content']
            self.fp.write(author + ':' + content)
            return item
    
        # 重写父类方法：只在开始爬虫的时候被调用一次
        def close_spider(self, spider):
            print("结束爬虫")
            self.fp.close()
    
    
    class MysqlPileLine(object):
        conn = None
        cursor = None
    
        def open_spider(self, spider):
            self.conn = pymysql.Connect(host='127.0.0.1', port=3306, user='root', password='root', db='test',
                                        charset='utf8')
    
        def process_item(self, item, spider):
            self.cursor = self.conn.cursor()
    
            try:
                self.cursor.execute('insert into test values("%s","%s")' % (item['author'], item['content']))
                self.conn.commit()
            except Exception as e:
                print(e)
                self.conn.rollback()
    
        def close_spider(self, spider):
            self.cursor.close()
            self.conn.close()
    ```

### 8.5、基于Spider的全站数据爬取

* 将网站中某板块下的全部页面对应的页面数据进行爬取

* 需求：爬取彼岸图网中的照片名称

  * 实现方法：

    * 将所有页面的url添加到start_urls（不推荐）

    * 手动发送请求：```yield scrapy.Request(url=new_url, callback=self.parse)```

      ```python
      import scrapy
      
      
      class NetbianSpider(scrapy.Spider):
          name = 'netbian'
          # allowed_domains = ['www.pic.netbian.com']
          start_urls = ['https://pic.netbian.com/4kfengjing/']
      
          # 生成一个通用的url模板
          url = 'https://pic.netbian.com/4kfengjing/index_%d.html'
          page_num = 2
      
          def parse(self, response):
              names = response.xpath('//*[@id="main"]/div[3]/ul/li/a/img/@alt').extract()
              for name in names:
                  print(name)
      
              if self.page_num <= 3:
                  new_url = format(self.url % self.page_num)
                  self.page_num += 1
                  # 手动发送请求
                  yield scrapy.Request(url=new_url, callback=self.parse)
      ```

### 8.6、scrapy五大核心组件

#### 8.6.1、引擎（Scrapy）

* 用来处理整个系统的数据流处理，触发事物（框架核心）

#### 8.6.2、调度器（Scheduler）

* 用来接收引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回，可以想象成一个URL的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址

#### 8.6.3、下载器(Downloader)

* 用于下载网页内容，并将网页内容返回给蜘蛛（Scrapy下载器是建立在twisted这个高效的异步模型上的）

#### 8.6.4、爬虫Spider

* 爬虫是主要干活的，用于从特定的网页中提取自己需要的信息，即所谓的实体。用户也可以从中提取出链接，让Scrapy继续抓取下一个页面

#### 8.6.5、项目管道

* 负责处理爬虫从网页中抽取的实体，主要功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据

### 8.7、请求传参

* 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）

* 需求：爬取boos直聘的岗位名称、岗位描述

* ```python
  # meta传递字典类型参数
  yield scrapy.Request(detail_url, callback=self.parse_detail, meta={'item': item})
  # 接收
  item = response.meta['item']
  ```

### 8.8、图片数据爬取

#### 8.8.1、图片数据爬取之ImagesPipeline

* 基于scrapy爬取字符串类型的数据和爬取图片类型数据区别

  * 字符串：只需要基于xpath进行解析且提交管道进行持久化存储
  * 图片：xpath解析出图片src的属性值。单独的对图片地址发起请求获取二进制类型的数据

* ImagesPipeline:

  * 只需要将img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制数据，并且进行持久化存储

* 使用流程

  * 需求：爬取站长素材中的高清图片

  * 数据解析：解析出图片地址

    ```python
    import scrapy
    from scrapy_demo.items import ImgsproItem
    
    
    class FirstSpider(scrapy.Spider):
        # 爬虫文件名称，爬虫源文件的一个唯一标识
        name = 'first'
        # 起始的url列表：该列表中存放的url会被scrapy自动进行请求的发送
        start_urls = ['https://sc.chinaz.com/tupian/']
    
        def parse(self, response):
            srcs = response.xpath('/html/body/div[3]/div[2]/div/img/@data-original').extract()
            for src in srcs:
                img_url = 'https://' + src
                item = ImgsproItem()
                item['img_url'] = img_url
    
                yield item
    ```

  * 将存储图片地址的item提交到指定管道类

    ```python
    yield item
    ```

  * 在管道文件类中自定义一个基于ImagesPipeLine的管道类

    * 重写三个方法

      get_media_requests()
      file_path()
      item_completed()

    * ```python
      # Define your item pipelines here
      #
      # Don't forget to add your pipeline to the ITEM_PIPELINES setting
      # See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
      
      
      # useful for handling different item types with a single interface
      import scrapy
      from itemadapter import ItemAdapter
      from scrapy.pipelines.images import ImagesPipeline
      
      
      class ImgsPipeline(ImagesPipeline):
          def get_media_requests(self, item, info):
              """
              根据图片地址进行图片数据的请求
              :param item:
              :param info:
              :return:
              """
              yield scrapy.Request(item['img_url'])
      
          def file_path(self, request, response=None, info=None, *, item=None):
              """
              指定图片存储的路径
              :param request:
              :param response:
              :param info:
              :param item:
              :return:
              """
              img_name = request.url.split('/')[-1]
              return img_name
      
          def item_completed(self, results, item, info):
              """
              返回给下一个即将被执行的管道类
              :param results:
              :param item:
              :param info:
              :return:
              """
              return item
      ```

  * 在配置文件中

    * 指定图片存储的目录：IMAGES_STORE = './imgs'

    * 指定开启的管道：自定制的管道类

      ```python
      ITEM_PIPELINES = {
          'scrapy_demo.pipelines.ImgsPipeline': 300
      }
      ```

### 8.9、中间件

* 下载中间件

  * 位置：在引擎与下载器之间

  * 作用：批量拦截到整个工程中所有的请求和响应

  * 拦截请求：

    * UA伪装
      * process_request
    * 代理IP
      * process_exception:return request

    ```python
    # Define here the models for your spider middleware
    #
    # See documentation in:
    # https://docs.scrapy.org/en/latest/topics/spider-middleware.html
    import random
    
    from scrapy import signals
    
    # useful for handling different item types with a single interface
    from itemadapter import is_item, ItemAdapter
    
    
    class MiddleSpiderMiddleware:
        # Not all methods need to be defined. If a method is not defined,
        # scrapy acts as if the spider middleware does not modify the
        # passed objects.
    
        @classmethod
        def from_crawler(cls, crawler):
            # This method is used by Scrapy to create your spiders.
            s = cls()
            crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
            return s
    
        def process_spider_input(self, response, spider):
            # Called for each response that goes through the spider
            # middleware and into the spider.
    
            # Should return None or raise an exception.
            return None
    
        def process_spider_output(self, response, result, spider):
            # Called with the results returned from the Spider, after
            # it has processed the response.
    
            # Must return an iterable of Request, or item objects.
            for i in result:
                yield i
    
        def process_spider_exception(self, response, exception, spider):
            # Called when a spider or process_spider_input() method
            # (from other spider middleware) raises an exception.
    
            # Should return either None or an iterable of Request or item objects.
            pass
    
        def process_start_requests(self, start_requests, spider):
            # Called with the start requests of the spider, and works
            # similarly to the process_spider_output() method, except
            # that it doesn’t have a response associated.
    
            # Must return only requests (not items).
            for r in start_requests:
                yield r
    
        def spider_opened(self, spider):
            spider.logger.info('Spider opened: %s' % spider.name)
    
    
    class MiddleDownloaderMiddleware:
        # Not all methods need to be defined. If a method is not defined,
        # scrapy acts as if the downloader middleware does not modify the
        # passed objects.
        user_agent_list = [
            "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 "
            "(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
            "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 "
            "(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
            "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 "
            "(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
            "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 "
            "(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
            "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 "
            "(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 "
            "(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
            "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 "
            "(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
            "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
            "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
            "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
            "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
            "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
            "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
            "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
            "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 "
            "(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 "
            "(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",
            "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 "
            "(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
        ]
    
        PROXY_http = [
            '223.96.90.216:8085'
        ]
    
        PROXY_https = [
            '', ''
        ]
    
        def process_request(self, request, spider):
            """
            拦截请求
            :param request:
            :param spider:
            :return:
            """
            # UA伪装
            request.headers['User-Agent'] = random.choice(self.user_agent_list)
            request.meta['proxy'] = 'http://101.200.127.149:3129'
            return None
    
        def process_response(self, request, response, spider):
            """
            拦截响应
            :param request:
            :param response:
            :param spider:
            :return:
            """
            # Called with the response returned from the downloader.
    
            # Must either;
            # - return a Response object
            # - return a Request object
            # - or raise IgnoreRequest
            return response
    
        def process_exception(self, request, exception, spider):
            """
            拦截发生异常的请求
            :param request:
            :param exception:
            :param spider:
            :return:
            """
            # 代理
            if request.url.split(':')[0] == 'http':
                request.meta['proxy'] = 'http://' + random.choice(self.PROXY_http)
            else:
                request.meta['proxy'] = 'https://' + random.choice(self.PROXY_https)
            return request
    ```

    ```python
    import scrapy
    
    
    class Middle1Spider(scrapy.Spider):
        name = 'middle1'
        # allowed_domains = ['www.baidu.com']
        start_urls = ['http://www.baidu.com/s?wd=ip']
    
        def parse(self, response):
            page_text = response.text
            with open('ip.html', 'w', encoding='utf8') as fp:
                fp.write(page_text)
    ```

    ```python
    DOWNLOADER_MIDDLEWARES = {
       'middle.middlewares.MiddleDownloaderMiddleware': 543,
    }
    ```

  * 拦截响应：

    * 篡改响应数据，响应对象

    * 需求：爬取网易新闻中的新闻数据（标题和内容）

      * 1. 通过网易新闻的首页解析出五大板块对应的详情页的url（没有动态加载）
        2. 每一个板块对应的新闻标题都是动态加载出来的(动态加载)
        3. 通过解析出每一条新闻详情页url获取详情页内容

    * 代码

      ```python
      import scrapy
      from selenium import webdriver
      from selenium.webdriver.chrome.service import Service
      
      
      class WangyiSpider(scrapy.Spider):
          name = 'wangyi'
          # allowed_domains = ['www.baidu.com']
          start_urls = ['https://news.163.com/']
          url = ''
      
          def __init__(self):
              s = Service('D:\project\python\scrapy_demo\chromedriver.exe')
              self.bro = webdriver.Chrome(service=s)
      
          def parse(self, response):
              self.url = response.xpath(
                  '//*[@id="index2016_wrap"]/div[2]/div[2]/div[2]/div[2]/div/ul/li[2]/a/@href').extract_first()
              yield scrapy.Request(self.url, callback=self.parse_model)
      
          def parse_model(self, response):
              # print(response)
              title = response.xpath(
                  '/html/body/div/div[3]/div[3]/div[1]/div[1]/div/ul/li/div/div[1]/div/div[1]/h3/a/text()').extract()
              print(title)
              self.url = response.xpath(
                  '/html/body/div/div[3]/div[3]/div[1]/div[1]/div/ul/li/div/div[1]/div/div[1]/h3/a/@href').extract()[0]
              yield scrapy.Request(self.url, callback=self.parse_detail)
      
          def parse_detail(self, response):
              content = response.xpath('//*[@id="content"]/div[2]').extract()[0]
              print(content)
      
          def close(self, spider):
              self.bro.quit()
      
      ```

      ```python
      # Define here the models for your spider middleware
      #
      # See documentation in:
      # https://docs.scrapy.org/en/latest/topics/spider-middleware.html
      
      from scrapy import signals
      from scrapy.http import HtmlResponse
      from time import sleep
      
      # useful for handling different item types with a single interface
      from itemadapter import is_item, ItemAdapter
      
      
      class WangyiproSpiderMiddleware:
          # Not all methods need to be defined. If a method is not defined,
          # scrapy acts as if the spider middleware does not modify the
          # passed objects.
      
          @classmethod
          def from_crawler(cls, crawler):
              # This method is used by Scrapy to create your spiders.
              s = cls()
              crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
              return s
      
          def process_spider_input(self, response, spider):
              # Called for each response that goes through the spider
              # middleware and into the spider.
      
              # Should return None or raise an exception.
              return None
      
          def process_spider_output(self, response, result, spider):
              # Called with the results returned from the Spider, after
              # it has processed the response.
      
              # Must return an iterable of Request, or item objects.
              for i in result:
                  yield i
      
          def process_spider_exception(self, response, exception, spider):
              # Called when a spider or process_spider_input() method
              # (from other spider middleware) raises an exception.
      
              # Should return either None or an iterable of Request or item objects.
              pass
      
          def process_start_requests(self, start_requests, spider):
              # Called with the start requests of the spider, and works
              # similarly to the process_spider_output() method, except
              # that it doesn’t have a response associated.
      
              # Must return only requests (not items).
              for r in start_requests:
                  yield r
      
          def spider_opened(self, spider):
              spider.logger.info('Spider opened: %s' % spider.name)
      
      
      class WangyiproDownloaderMiddleware:
          # Not all methods need to be defined. If a method is not defined,
          # scrapy acts as if the downloader middleware does not modify the
          # passed objects.
      
          def process_request(self, request, spider):
              print(request.url)
              return None
      
          def process_response(self, request, response, spider):
              # 浏览器对象
              bro = spider.bro
      
              if request.url == spider.url:
                  bro.get(request.url)
                  sleep(1)
                  page_text = bro.page_source
                  new_response = HtmlResponse(url=request.url, body=page_text, encoding='utf-8', request=request)
                  return new_response
              return response
      
          def process_exception(self, request, exception, spider):
              print('异常')
              pass
      ```

      ```python
      DOWNLOADER_MIDDLEWARES = {
         'wangyiPro.middlewares.WangyiproDownloaderMiddleware': 543,
      }
      ```

* 爬虫中间件

  * 在引擎与爬虫之间



### 8.10、CrawlSpider

#### 8.10.1、简介

* CrawlSpider是Spider的一个子类，用于全站数据爬取

* 全站数据爬取的两种方式
  * 基于Spider：手动请求
  * 基于CrawlSpider

#### 8.10.2、使用方法

* 创建一个工程

* 创建爬虫文件（基于CrawSpider）

  ```scrapy genspider -t crawl name www.xxx.com```

* 链接提取器

  ```python
  # 链接提取器：根据指定规则(allow="正则表达式")进行指定链接提取
  ```

* 规则解析器

  * 将链接提取器提取到的链接进行指定规则(callback)的解析操作
  * follow = True：可以将链接提取器 继续作用到 链接提取器提取到的链接 所对应的页面中
  * xpath中不能出现tbody标签

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class SunSpider(CrawlSpider):
    name = 'sun'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://wz.sun0769.com/political/index/politicsNewest']

    # 链接提取器：根据指定规则(allow="正则表达式")进行指定链接提取
    link = LinkExtractor(allow=r'id=1&page=\d+')

    # 规则解析器
    rules = (
        # 规则解析器：将链接提取器提取到的链接进行指定规则(callback)的解析操作
        Rule(link, callback='parse_item', follow=False),
    )

    def parse_item(self, response):
        print(response)
        # item = {}
        # # item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        # # item['name'] = response.xpath('//div[@id="name"]').get()
        # # item['description'] = response.xpath('//div[@id="description"]').get()
        # return item
```



## 9、分布式爬虫

### 9.1、概念

* 需要搭建一个分布式的集群，让其对一组资源进行分布联合爬取
* 作用：提升爬取数据的效率

### 9.2、分布式实现

* 安装pip install scrapy-redis组件

* 原生scrapy不能实现分布式爬虫，必须结合scrapy-redis组件进行分布式实现

* scrapy-redis可以给原生scrapy框架提供可以被共享的管道和调度器

* 实现流程

  * 创建工程

  * 创建一个一个基于CrawSpider的爬虫文件

  * 修改当前的爬虫文件

    * 导入包：from scrapy_redis.spiders import RedisCrawlSpider

    * 注释掉allowed_domains、start_urls

    * 添加一个新属性：redis_key = 'sun' 可以被共享的调度器队列的名称

    * 编写数据解析相关的操作

    * 修改爬虫类父类为RedisCrawlSpider

      ```python
      import scrapy
      from scrapy.linkextractors import LinkExtractor
      from scrapy.spiders import CrawlSpider, Rule
      
      from scrapy_redis.spiders import RedisCrawlSpider
      
      
      class SunSpider(RedisCrawlSpider):
          name = 'sun'
          # allowed_domains = ['www.xxx.com']
          # start_urls = ['https://wz.sun0769.com/political/index/politicsNewest']
          redis_key = 'sun'
      
          # 链接提取器：根据指定规则(allow="正则表达式")进行指定链接提取
          link = LinkExtractor(allow=r'id=1&page=\d+')
      
          # 规则解析器
          rules = (
              # 规则解析器：将链接提取器提取到的链接进行指定规则(callback)的解析操作
              Rule(link, callback='parse_item', follow=True),
          )
      
          def parse_item(self, response):
              print(response)
      ```

  * 修改配置文件

    * 指定使用可以被共享的管道：

      ```python
      ITEM_PIPELINES = {
         'scrapy_redis.pipelines.RedisPipeline': 400
      }
      ```

    * 指定调度器

      ```python
      DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
      # 使用scrapy_redis组件自己的调度器
      SCHEDULER = 'scrapy_redis.scheduler.Scheduler'
      # 配置调度器是否要持久化
      SCHEDULER_PERSIST = True
      ```

    * 指定redis服务器

      ```python
      REDIS_HOST = '127.0.0.1'
      REDIS_PORT = 6379
      ```

  * redis相关操作配置

    * 配置redis的配置文件redis.conf

      * 注释掉```bind 127.0.0.1```
      * 关闭保护模式 ```protected-mode yes 改为no```

    * 启动redis服务

      ```shell
      ./redis-server ../redis.conf
      ```

    * 启动客户端

      ```shell
      ./redis-cli
      ```

  * 执行工程

    ```python
    scrapy runspider xxx.py
    ```

  * 向调度器的队列中放入一个其实的url

    * redis客户端

      ```shell
      lpush xxx www.xxx.com
      ```

  * 查看数据

    ```shell
    # 查看全部key值
    keys*
    # fbs:items中存储爬取到的item
    lrange fbs:items
    ```

    
